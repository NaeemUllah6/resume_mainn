<!DOCTYPE html>
<html lang="en">

<head>
  <title>João Morais</title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <link rel="stylesheet" href="../../assets/css/main.css" />
  <link rel="icon" href="../../images/coffee.png" type="image/png" />

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" />
  <noscript>
    <link rel="stylesheet" href="../../assets/css/noscript.css" />
  </noscript>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.css" />
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      corePlugins: {
        preflight: false,
      },
    };
  </script>
</head>

<body>
  <div id="wrapper">
    <main class="publicationMain">
      <section class="publicationArticle">
        <a href="/" class="goback">
          <i class="fa-solid fa-arrow-left-long"></i>
        </a>
        <div class="text-center">
          <div class="italic">
            IEEE Communications Magazine, 2023
          </div>
          <div style="border-bottom: 1px solid gray; padding-bottom: 10px" class="text-[1.8rem]">
            DeepSense 6G: A Large-Scale Real-World Multi-Modal Sensing and Communication Dataset
          </div>
          <div id="authors" style="
                    margin-top: 2ex;
                    font-size: 1.2em;
                    flex-flow: wrap;
                    display: flex;
                    justify-content: center;
                  ">
            <span style="
                      font-variant: small-caps;
                      font-size: larger;
                      white-space: nowrap;
                      width: fit-content;
                      margin: 0px 2ex;
                    ">Ahmed Alkhateeb<span class="super"
                style="margin-left: 5px; white-space: nowrap"></span></span><span style="
                      font-variant: small-caps;
                      font-size: larger;
                      white-space: nowrap;
                      width: fit-content;
                      margin: 0px 2ex;
                    ">hmed Alkhateeb<span class="super" style="margin-left: 5px; white-space: nowrap"></span></span>

            <span style="
                      font-variant: small-caps;
                      font-size: larger;
                      white-space: nowrap;
                      width: fit-content;
                      margin: 0px 2ex;
                    ">Tawfik Osman<span class="super" style="margin-left: 5px; white-space: nowrap"></span></span>

            <span style="
                      font-variant: small-caps;
                      font-size: larger;
                      white-space: nowrap;
                      width: fit-content;
                      margin: 0px 2ex;
                    ">Andrew Hredzak<span class="super" style="margin-left: 5px; white-space: nowrap"></span></span>
            <span style="
                      font-variant: small-caps;
                      font-size: larger;
                      white-space: nowrap;
                      width: fit-content;
                      margin: 0px 2ex;
                    ">João Morais<span class="super" style="margin-left: 5px; white-space: nowrap"></span></span>
            <span style="
                      font-variant: small-caps;
                      font-size: larger;
                      white-space: nowrap;
                      width: fit-content;
                      margin: 0px 2ex;
                    ">Umut Demirhan<span class="super" style="margin-left: 5px; white-space: nowrap"></span></span>
            <span style="
                      font-variant: small-caps;
                      font-size: larger;
                      white-space: nowrap;
                      width: fit-content;
                      margin: 0px 2ex;
                    ">Nikhil Srinivas<span class="super" style="margin-left: 5px; white-space: nowrap"></span></span>
          </div>
          <div class="flex gap-8 justify-center" style="margin-top: 2ex">
            <span class="spanMobile"><span><span class="super" style="margin-right: 5px"></span>
                <span>Arizona State University</span></span></span><span class="spanMobile"><span><span class="super"
                  style="margin-right: 5px; font-size: 1"></span>
              </span></span>
          </div>
          <!-- <div data-fancybox="gallery"
                        data-src="https://#.me/publications/nlos-scattering-media/figures/teaser.jpg">
                        <img src="https://#.me/publications/nlos-scattering-media/figures/teaser.jpg" alt=""
                            class="rounded-md w-full my-4" />
                    </div> -->
        </div>
        <figcaption>
          <p class="italic" style="
                text-align: justify;
                text-justify: inter-character;
                text-indent: 1.5rem;
                margin: 0 0 0.5rem 0;
              ">
            <!-- <strong> Scene setup: </strong> The scene is hidden behind a
                        diffuser, with the scene being submerged in a scattering medium.
                        <strong> Reconstructions of experimental data: </strong> Each
                        column is a different scene. Top: images of the scenes behind the
                        diffuser. Middle: Lindell and Wetzstein reconstructions (CDT).
                        Bottom: our reconstructions using Phasor Fields. The higher
                        frequency of CDT is related to the deconvolution to compensate
                        scattering at the diffuser.
                    </p> -->
        </figcaption>
        <div class="resources_publication">
          <h2>Resources</h2>
          <a class="button_resources" href="https://arxiv.org/pdf/2211.09769"><i
              class="button_icon fa fa-file-pdf"></i><span> <!-- -->Paper (3.7 MB)<!-- --> </span>
          </a>
          <a class="button_resources" href="https://ieeexplore.ieee.org/abstract/document/10144504"><i
              class="button_icon fa fa-quote-right"></i><span>
              <!-- -->Cite
              <!-- -->
            </span>
          </a>
        </div>
        <h2 id="Abstract">Abstract</h2>
        <p style="
              text-align: justify;
              text-justify: inter-character;
              text-indent: 1.5rem;
              margin: 0 0 0.5rem 0;
            ">
          This article presents the DeepSense 6G data-set, which is a large-scale dataset based on real-world
          measurements of co-existing multi-modal sensing and communication data. The Deep-Sense 6G dataset is built to
          advance deep learning research in a wide range of applications in the intersection of multi-modal sensing,
          communication, and positioning. This article provides a detailed overview of the DeepSense dataset structure,
          adopted testbeds, data collection and processing methodology, deployment scenarios, and example applications,
          with the objective of facilitating the adoption and reproducibility of multi-modal sensing and communication
          datasets.
        </p>
        <h2>Videos</h2>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/fJ5OomvILRc?si=sqCer4JAvLGEnjRA"
          title="YouTube video player" frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        <h2 id="Figures" class="py-4">Figures</h2>
        <div class="lg:columns-3 columns-2 gap-2 overflow-hidden">
          <figure data-fancybox="gallery" data-src='./gallery/55.jpg' data-caption="Fig. 3. The DeepSense 6G dataset comprises 40+ scenarios with realistic datasets to encourage the development of novel applications. The dataset is collected
                      to enable various deployment scenarios. This figure presents the different deployment scenarios included in the DeepSense 6G dataset. As shown here, the
                      dataset covers a wide range of deployments such as vehicle to infrastructure, vehicle to vehicle, drone communication, reconfigurable intelligent surfaces,
                      pedestrians, indoor use cases, to name a few.
                      ">
            <img src='./gallery/55.jpg' alt="" class="rounded-md w-full" loading="lazy" />
          </figure>


          <figure data-fancybox="gallery" data-src='./gallery/54.jpg' data-caption="Fig. 2. The DeepSense 6G dataset is a collection of scenarios, where each scenario consists of the multi-modal sensing and communication data collected in
                      one long data collection session. This figure outlines the different steps required to generate the final development (scenario) dataset after each data collection
                      session. The process comprises three steps: (i) Real-world data collection, (ii) data processing, and (iii) data filtering.
                      ">
            <img src='./gallery/54.jpg' alt="" class="rounded-md w-full h-[150px]" loading="lazy" />
          </figure>

          <figure data-fancybox="gallery" data-src='./gallery/56.jpg'
            data-caption="Fig. 4. This figure plots the confusion matrices for the top-1 predicted beam indices. It shows the following two cases: (a) Training and testing on scenario
                      5 development dataset and (b) training on scenario 1 data and evaluating on scenario 5 test dataset. The figure on the right, (b), shows that even without no
                      training data of scenario 5, the proposed solution can predict the optimal beam indices with sufficient accuracy">
            <img src='./gallery/56.jpg' alt="" class="rounded-md w-full h-[150px]" loading="lazy" />
          </figure>
          <figure data-fancybox="gallery" data-src='./gallery/56.jpg'
            data-caption="Fig. 1. This figure presents the DeepSense 6G testbed 1 and the different sensing modalities. It consists of two units: Unit 1 (a stationary unit), which acts
                      as the basestation, and unit 2 (a vehicle), which represents the mobile user. It also shows the final scenario structure, where the data is eventually stored as a
                      sequence of data groups with each group containing data collected from all the sensors at the same sampling interval.">
            <img src='./gallery/graph.png' alt="" class="rounded-md w-full" loading="lazy" />
          </figure>
          <figure data-fancybox="gallery" data-src='./gallery/56.jpg'
            data-caption="Fig. 1. This figure presents the DeepSense 6G testbed 1 and the different sensing modalities. It consists of two units: Unit 1 (a stationary unit), which acts
                      as the basestation, and unit 2 (a vehicle), which represents the mobile user. It also shows the final scenario structure, where the data is eventually stored as a
                      sequence of data groups with each group containing data collected from all the sensors at the same sampling interval.">
            <img src='./gallery/testbeds.jpg' alt="" class="rounded-md w-full" loading="lazy" />
          </figure>
        </div>


        <!-- <h2 id="Figures" class="pt-8">Bibtex</h2> -->
        <div class="code-container">
          <div class="copy-btn"><i class="fa-regular fa-copy"></i></div>
          <pre>
            <code>@ARTICLE{10144504,
    author={Alkhateeb, Ahmed and Charan, Gouranga and Osman, Tawfik and Hredzak, Andrew and Morais, Joao and Demirhan, Umut and Srinivas, Nikhil},
    journal={IEEE Communications Magazine}, 
    title={DeepSense 6G: A Large-Scale Real-World Multi-Modal Sensing and Communication Dataset}, 
    year={2023},
    volume={61},
    number={9},
    pages={122-128},
    keywords={Sensors;6G mobile communication;Data collection;Millimeter wave communication;Laser radar;Wireless communication;Global Positioning System},
    doi={10.1109/MCOM.006.2200730}}
              </code>
        </pre>
        </div>
      </section>
    </main>
  </div>
  <!-- BG -->
  <div id="bg" class="publications"></div>

  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.umd.js"></script>

  <script src="../../assets/js/publication.js"></script>
  <!-- <script src="../../assets/js/main.js"></script> -->
</body>

</html>